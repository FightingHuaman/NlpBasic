输入矩阵是由离散特征值构成的。值得一提的是，初始状态数据集为空集，当数据集自动累计获取到设定值，作为深度残差网络的输入数据开始训练，训练结束后，数据集会进行更新操作，即按照一定比例删除和加入数据条目，再进行下一轮训练。

本文给出的网络结构均采用1*1卷积，按照设置的卷积核大小对区域内元素进行卷积运算。在本文中，输入矩阵是由离散特征值构成的，故采用size大于1的卷积核搭建网络很有可能造成信息丢失，所以采用传统卷积网络是不可行的。让网络根据需要能够更灵活的控制数据的depth的同时，也有利于增加深层网络的宽度。在网络表达能力得以保障的基础上，加入1×1卷积后可以降低输入的通道数，卷积核参数、运算复杂度也就跟着降下来了，故本文采用了大量卷积核大小为1的卷积层。此外，卷积层之后经过激励层，1*1的卷积在前一层的学习表示上添加了非线性激励（ non-linear activation ），例如激励函数Relu，可进一步提升网络的表达能力。

池化层不会改变矩阵的深度，但是可以缩小矩阵的大小。通过池化层可以进一步缩小最后全连接层中节点的个数，从而达到减少整个网络参数的目的。本文给出的网络结构同时采用了最大化池化层和平均池化层分别进行最大值和平均值计算。

普通的深度前馈网络难以优化。除了深度，所加层使得training和validation的错误率增加，即使用上了batch normalization也是如此。残差神经网络由于存在shorcut connections，网络间的数据流通更为顺畅。ResNet作者认为深度残差网络不太可能由于梯度消失而形成欠拟合，因为这在batch normalized network中就很难出现。残差网络结构的解决方案是，增加卷积层输出求和的捷径连接。也就是在标准的前馈卷积网络上，加一个跳跃绕过一些层的连接。每绕过一层就产生一个残差块(residual block)，卷积层预测加输入张量的残差。
在经过多轮卷积层和池化层的处理之后，在网络最后是有全连接层给出结果。经过多轮卷积层和池化层的处理之后，可以认为输入数据信息已经被抽象成了信息含量更高的特征，在特征提取完毕后，最后由全连接层给出预测结果。


用soft policies来替换确定性策略，使所有的动作都有可能被执行。比如其中的一种方法是ε-greedy policy，即在所有的状态下，用1-ε的概率来执行当前的最优动作，ε的概率来执行其他动作。这样我们就可以获得所有动作的估计值，然后通过慢慢减少ε值，最终使算法收敛，并得到最优策略。此处MC控制，我们使用exploring start，即仅在第一步令所有的动作都有一个非零的概率被选中。
　
　该方法的好处就是不需要环境模型，可以从经验中直接学到策略。它对所有状态s的估计都是独立的，而不依赖与其他状态的值函数。在很多时候，我们不需要对所有状态值进行估计，这种情况下蒙特卡罗方法就十分适用。
　
初始状态数据集为空集，当数据集自动累计获取到一万条数据，作为深度残差网络的输入数据开始训练，训练结束后，数据集会进行更新操作，即删除百分之八十留下百分之二十，再重新累加录入新的数据条目至一万条，再进行下一轮训练。
　




