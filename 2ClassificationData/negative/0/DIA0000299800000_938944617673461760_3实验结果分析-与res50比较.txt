以下两张图分别为深度残差网络在训练集合和测试集合上的的误差对比，相比较于传统的训练过程，我们的框架不存在固定的数据集，而是模拟实际环境，在线根据不同分布产生训练数据，不断输入至recorder，当recorder中存储的数据条目达到预设值就对网络进行训练，每次训练结束后都会清空recorder，重新开始记录数据。

图一是res50和res50-variant在训练过程的误差曲线，可以看出虽然原res50误差下降稍快于我们改动后的res50-variant，但res50-variant最终收敛的误差界限远小于原res50.可以得出res50-variant相较于原网络误差更小。
图二对比原res50和res50-variant （figure for validation error）的验证误差，可以看到 原res50的预测误差比较稳定，说明res50-variant更加容易受到环境影响，这是由于为了避免离散信息混合我们使用的全部卷积核大小均为1，感受野（receptive field）过小 造成了网络模型缺乏对输入矩阵的整体理解，accordingly 对单个数据的变化也会更敏感。 在大多数情况下res50-variant预测时的误差小于原res50。

使用不同深度残差网络模型的框架的性能评估
step对应的是整体框架进行一次输出

图三是CHR随step变化而变化的曲线。在我们的加强学习框架下，进行补偿后的两者都能很快的达到99%以上，但res50-variant 在98%的情况下是高于原res50的.

图四是ECR随step变化的曲线，这个数值越高代表冗余块数越少。冗余块数由以下两个factor共同决定：（1）深度残差网络预测准确程度。（2）强化学习的策略评估。

在这张图中可以观察得到以下三点：
（1）尽管原res50能够更快的学习到预测的整体趋势，收敛较res50-variant更快，但收敛精度不足导致补偿块数一直较多，ECR保持在较低的水平上。
（2）res50-variant在step为10000之前，网络模型预测精度在短时间内先越过设定阈值，停止训练，与此同时，强化学习通过迭代找到最优策略。在图四中表现为上升趋势。
（3）波动的产生由于两个方面共同导致，a.强化学习中策略收敛的方向是更少的补偿块数，但若由于补偿块数过少导致CHR小于tao便会选择补偿块数较多的策略，这构成了波动的一部分原因。b.强化学习中包含一定探索率的贪婪搜索策略，即强化学习会按照一定比率随机选择策略，这也是波动的原因。c.res50-variant不能够做到100%准确率的预测。

res50-variant表现好于res50，以上综述 res50-variant相较于res50更适用于我们的框架以及对应的问题。

在整体框架中res50-variant表现优于原res50是由于全部卷积层都采用了大小为1*1的filter同时strides也为1，而且在将网络分支融合时没有进行加合操作而是连接操作，尽力避免离散信息混合在一起造成信息丢失。由于我们输入的矩阵指定类型的数据处于矩阵的固定位置，池化的filter大小也为1*1，stride也为1.这样就避免了位置信息丢失从而获得了更高的精度，更小的误差。同时也由于以上改动造成了感受野狭小，网络模型牺牲了鲁棒性和对整个输入矩阵的较为通泛的理解。
