
     随着半导体技术的巨大发展，单张芯片中集成了数以亿计的晶体管，芯片中核心之间的通信距离达到微米级。随之而来的摩尔极限使得通过增加晶体管数量这一方法难以提升芯片整体的计算能力。芯片中的计算核心需要大量的通信资源，并且芯片能耗逐渐成为制约系统整体性能的主要因素。学者提出了将多种计算核心集成于一张芯片的片上网络技术来解决上述问题。与传统的基于总线互连结构相比，片上网络具有很好的扩展性和并行性。在NoC(network-on-chip)体系结构中，不同的互连模块通过路由节点连接，每个节点可以是通用处理器、DSP或者存储系统。NoC中的数据被拆分成数据分片(flit)的形式在网络中进行传输。
     为了进一步提高计算性能，近年来流行在芯片中集成GPU和CPU计算核心。CPU和GPU具有完全不同的通信特性，为了保证片上网络各个部件间协同工作，部件间的通信质量至关重要。CPU被设计成在保持应用总体按需执行的前提下，采用复杂的控制逻辑以允许单个线程并行执行甚至乱序执行指令。相反，GPU追求较高的吞吐率，拥有大量的线程，当其中一些线程正在等待存储器存取或是算数计算时，GPU可以用其余的线程继续进行剩下的工作。这种设计风格在允许个别线程执行时间延长的情况下，增大了总体的程序执行吞吐量。图1是一个典型的GPU-CPU二维Mesh异构片上网络系统实例图。GPU核心、CPU核心、末级高速缓冲存储器、存储控制器都可以看作片上网络中一个独立的节点，每个节点都与一个路由器相连。路由器负责处理节点间的通信任务，每个路由器有东、西、南、北、本地五个输入输出端口，并相应地配置了输入缓冲区。节点间片上通信时由其中一个节点发起请求，并通过解码、仲裁、链路传输等一系列过程最终到达目的节点。

图1：GPU-CPU二维Mesh异构片上网络系统实例图
     为了保证片上网络的通信服务质量和性能，需要尽可能降低节点间的通信延迟。一般来说，在传统通用片上网络中，路由缓冲区资源越多，系统性能越好，通信延迟越低。但是随着业务场景的不断提出，真实世界中的应用程序变得十分复杂。仅仅集成了CPU计算核心的传统片上网络并不能满足应用程序要求的吞吐率和并行计算能力，于是集成了GPU和CPU的异构片上网络逐渐流行起来。在GPU-CPU异构片上网络中，CPU和GPU处理的任务特性具有明显差异，这就导致了片上网络每个互连通道中数据通信量不均衡。将通用片上网络中所采取的平均分配缓冲区给每个核心的负载均衡策略应用在GPU-CPU异构体系片上网络中，会使得通信更加频繁的节点由于路由缓冲区严重不足，该条链路中的通信延迟会显著增加。负责执行复杂控制逻辑的CPU核心吞吐量远小于GPU核心，平均分配缓冲区的策略会造成CPU节点路由的缓冲区大量闲置，因而产生大量的静态功耗，并且闲置的缓冲区没有任何作用。同时由于GPU拥有大量的线程，通信更加频繁，这将导致目标链路出现严重的拥堵现象。因此传统的缓冲区分配策略不适用于所提出的GPU-CPU异构片上网络。另外，有研究指出片上网络中的缓冲区大小显著影响片上网络的能耗，因为路由缓冲区占据了芯片很大一部分面积。据文献统计，片上网络中的每个缓冲区增加一个数据分片单位大小，芯片总面积相应增大约30%，这会带来了极大的静态功耗。有学者设计了无缓冲路由和有缓冲路由组合的方式来降低芯片总能耗，但这样做只能在片上网络通信压力较小的情况下取得很好的效果。一旦通信压力增大，片上网络中的通信延迟将显著增加，通信链路变得拥堵，降低系统整体性能和可靠性。由此可见，如何将缓冲区合理地分配给不同特性的计算节点变得十分重要。在这种背景下，本文针对特定类型的应用程序，探索了CPU、GPU异构体系片上网络缓冲区的分配策略，使得系统在运行过程中得到性能、能耗、以及成本的折中。

     为了应对真实世界中日益复杂的应用程序，从而提出适合异构片上网络缓冲区的分配策略，我们首先分析不同的基准测试应用程序的片上网络流量特征，并统计片上网络中各节点路由缓冲区的使用情况。实验仿真的平台选用gem5gpu模拟器，它具有很好的模块化，并实现了内存模型并且能够良好的仿真CPU-GPU异构片上网络的各种配置，具有很强的适应性，功耗模块选用GPU-watch进行片上网络功耗分析。
     表1为CPU-GPU异构片上网络的配置。为了模拟集成了CPU核心和GPU核心的异构片上网络架构，CPU处理器采用Intel的Sandy Bridge，与之相配的是出自NVIDIA的高端GPU处理器 Fermi ，这是一种流式多处理器（streaming multiprocessor）。片上网络采用XY确定性路由算法（XY deterministic routing algorithm），因为复杂的路由算法需要复杂的通信协议及电路元件，这样做减少了设计的复杂度的同时减少了芯片面积,降低能耗。文章采用常规的4×4 Mesh结构，共包含16个节点。节点包含4个CPU核心、6个GPU核心，4个末级高速缓冲存储器及2个内存控制器，如图2所示。由于拓扑结构不是本文的研究重点，本文根据前人的结论选用了该拓扑结构。片上网络中CPU和GPU主要与内存控制器和末级高速缓冲存储器进行数据通信，因此将内存控制器和末级高速缓冲存储器放在芯片的中央，这样可以减少链路的通信压力。
     本文选用CUDA GPGPU基准测试程序以及CPU SPEC 2006基准测试程序进行实验。CUDA GPGPU基准测试程序来自NVidia CUDA SDK和Rodinia。我们根据程序每千周期（packet per kilocycle, PKC）向片上网络中注入数据包的数量作为指标将应用程序进行分类。在gem5gpu模拟器中，每一个CPU核心挂载一个CPU基准测试程序，所有GPU共同执行一个基准测试程序。 
     表2分别展示了实验中选择的基准测试应用程序的分类结果。我们将GPU基准测试程序分为三类，以基准测试应用程序的PKC作为指标，当PKC<100 时，该应用程序被看作是低注入率型应用程序（Packet Injection Rate）; 当240>PKC >100 时，该应用程序被看作是是中等注入率型应用程序。当PKC>240时，该应用程序被认为至高注入率型应用程序。对于CPU的基准测试应用程序，当PKC<35时，该应用程序被认为是低注入率型应用程序；当PKC>35时，该应用程序被认为是高注入率型应用程序。
     根据基准测试程序划分结果，进行三组实验，每组实验分别选取1个GPU应用程序和4个CPU应用程序作为工作负载（workload），在实验选定的的GPU-CPU异构片上网络中运行。在这三组实验中，GPU的工作负载类型的选择各不相同。由于计算核心可以使用本地内存来存储数据包，内存容量相对于路由器缓冲区大小可以视为无限，因此我们假设片上网络各个节点路由器中本地通道输入缓冲区的大小是无限。基于以上假设，我们为每个节点的东、西、南、北四个通道的输入端口平均分配了大小为11×fs大小的缓冲区，其中fs为每一个数据分片的大小FS（flit size），片上网络中缓冲区总容量为528×fs。本文统计了片上网络运行不同基准测试程序组合时每个节点路由器各输入通道缓冲区的使用情况，并根据缓冲区的平均占用率绘制了热点图，图中方格分别代表了图1中Mesh拓扑结构中相对位置的计算节点，颜色越深代表该节点路有缓冲区平均占用率越高，如图2所示：
        
(a)                    (b)			    	  (c)
图2 HNOC 路由器缓冲区占用率
     根据统计结果发现，片上网络中不同类型节点输入端口缓冲区的平均占用率差别很大，并且不同基准测试程序组合呈现出了不同的热点区域（hot spot）。图2(a)中热点区域热点区域为（0,2）、(0,3)、（2,2）三个节点，图(b)中热点区域为(1,2)、(1,3)两个节点，图（c）中热点区域为（1,2）、（3,2）两个节点。根据热点分布的节点类型可知，在CPU-GPU异构片上网络中，通信压力较大的链路区域主要集中在GPU核心、内存控制器和末级高速缓冲存储器中之中。由于GPU-CPU片上网络节点的异构性，片上网络中的流量模式也不同于传统片上网络，呈现明显的不均匀现象，并且不同的应用程序流量模式差异明显。在传统的片上网络中，为了追求更高的性能，片上网络路由器中被集成了越来越多的缓冲区，造成了片上网络功耗、散热不断增加，进而限制了片上网络性能的进一步提升。在异构网络中也有类似的问题，但是由于流量模式的不均匀性，GPU、LLC、MC节点的互连通信压力很大，而CPU节点的缓冲区由于应用程序特性往往缓冲区占用率不高，节点路由器中空闲的缓冲区没有被充分利用，造成大量的静态功耗，影响系统整体性能和功耗。
	为了更好地说明路由节点缓冲区分配策略对于片上网络性能的影响，我们将片上网络中各节点路由器分为三类，第一类连接CPU核心，第二类连接末级高速缓冲存储器或内存控制器，第三类连接GPU核心。分别统计三类路由器在配置不同大小缓冲区时，相关通信链路PKC的大小与系统平均数据包通信延迟的关系。系统平均数据包通信延迟被定义为从数据包产生到片上网络中最后一个数据分片（data flit）被传递到目的节点的平均时间。为了使统计结果更加准确，实验忽略了片上网络运行初始阶段的200个时钟周期（cycle）内的数据包传输，因为通信链路通信到达稳定阶段需要一定时间。
图3   不同类型节点缓冲区大小与系统平均数据包通信延迟的关系

     从图3的实验结果可以看出，片上网络中三类路由器随着缓冲区大小的增加，系统在不同数据包注入率（PKC）下平均延迟都呈下降趋势，但是下降的程度各不相同。由于CPU基准测试程序的PKC远小于GPU基准测试程序的PKC，因此通过增大CPU相关通信链路的缓冲区容量，对于系统整体的平均通信延迟影响十分有限。例如，对于第一类节点来说，当PKC等于120时，系统平均延迟一直维持在1300左右，此时缓冲区大于5时对于PKC的影响并不明显。第二类和第三类路由节点的通信压力较大，当PKC增大时，通过增加路由输入缓冲区的容量可以显著减小系统平均数据包通信延迟。例如，PKC为500时，当GPU节点路由输入缓冲区大小为5×fs时，系统平均延迟为12351cycles,而当路由输入缓冲区大小为11×fs时，系统平均数据包通信延迟仅为3782cycles，系统平均延迟下降了64.3%。同时PKC为500时，当MC/LLC节点路由缓冲区大小为5×fs时，系统平均数据包通信延迟为9508cycles，而当MC/LLC节点路由器缓冲区大小为11×fs时，系统平均数据包通信延迟为2782cycles, 系统平均延迟下降了70.7%。如图5所示。由此可见由于片上网络的异构性，将大量的缓冲区资源分配CPU核心所带来的性能提升并不明显。当CPU节点路由器缓冲区容量到达一个临界值后，提升第一类计算节点的缓冲区大小对于系统平均延迟的影响十分微弱。因为此时片上网络中的拥堵链路主要存在于第二类和第三类节点所对应的通信链路中，由于这些链路中缓冲区严重不足，系统平均数据包通信延迟一直维持在一个很高的水平。提升这两类节点链路中的输入缓冲区容量，系统平均数据包通信延迟将显著下降。
     由此可见，在缓冲区资源有限的情况下，为通信压力更大的链路分配更多的缓冲区资源是十分有效的。一方面非均匀分配缓冲区的方法可以更加合理的分配缓冲资源，将资源分配给最需要的节点，显著提升系统的整体性能。另一方面减少通信压力较小的链路中的缓冲区意味着减少了芯片的面积，这将带来可观的能耗降低。为了验证我们的观点，我们在选定的4×4Mesh结构的GPU-CPU异构片上网络中为第一类路由节点（CPU）的缓冲区分配不同容量的缓冲资源，统计系统整体能耗的变化。实验结果如图4所示。随着缓冲区的增加，系统平均总能耗呈现上升趋势。又由图3实验可知，当第一类路由节点缓冲区大小超过临界值5×fs时，通过提升缓冲区容量对于性能的增益十分微小。为了同时权衡系统性能、能耗、以及成本，本文将第一类路由节点缓冲区容量限制在5×fs，系统总能耗相比缓冲区容量为11×fs时平均降低了23.3%。这样既保证了系统性能没有明显下降，也减少了第一类路由节点的面积，因而减少了静态功耗和电路复杂度。
     为了进一步提高芯片整体性能，降低系统平均数据包通信延迟，本文在片上网络路由器缓冲区总容量一定的基础上，为通信量大、负载重的链路分配更多的缓冲区资源。由图3（a）和图4可知，第一类路由节点缓冲区大小为5×fs时，可以同时保证系统的性能、能耗以及成本保持在一个合理的范围内。因此当片上网络路由器缓冲区总量为528×fs时，我们为第二第三类路由器各通道的输入缓冲区容量设置为13×fs，第一类路由器各通道的输入缓冲区容量设置为5×fs。运行不同组合的基准测试程序，并与传统平均分配片上缓冲资源策略作对比，统计每个配置下系统数据包平均通信延迟。如图5所示,PKC为500时，当GPU/MC/LLC节点路由输入缓冲区大小为11×fs时,系统平均数据包通信延迟为3794cycles。当GPU/MC/LLC节点路由输入缓冲区大小为13×fs时，系统平均数据包通信延迟为2078cycles,系统平均数据包通信延迟下降了44%。图6展示了传统缓冲区平均分配策略与本文提出的分配方案的系统平均数据包通信延迟的对比。当PKC为480时，传统缓冲区平均分配策略为每一个节点路由器分配容量为11×fs大小的缓冲区，此时系统平均数据包通信延迟仅为3748cycles，本文提出的策略与之相比，系统平均延迟下降了44.6%。
